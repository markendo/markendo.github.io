<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Mark Endo</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
    
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">

<link rel="manifest" href="/site.webmanifest">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>

  <body>

<div class="article-meta">
<h1><span class="title">BioXtract: Learning Biomedical Knowledge From General and Random Data</span></h1>


</div>

<main>
<p>Abstract: The privacy of medical documents and protected healthcare information can oftentimes limit the accessibility of accurate biomedical natural language processing
models. Distillation can be used to transfer knowledge from these models, but it
typically relies on having related data to distill on. In this work, we investigate the
distillation of BERT-based biomedical models using transfer datasets from varying
domains, including general data, randomized general data, and biomedical data.
We find that general data can be used to learn task-specific biomedical knowledge,
especially when we can initialize student models with similar weights to the teacher.
We observe that randomized general data can also be used to transfer knowledge,
but it is not as effective as general data. We hope that our findings bring attention
to both the benefits and potential dangers of the widespread use of mixed-domain
pretraining in NLP, particularly relating to models that continue their pretraining
process on private data.</p>

</main>

  <footer>
  

  
  
  
  </footer>
  </body>
</html>

